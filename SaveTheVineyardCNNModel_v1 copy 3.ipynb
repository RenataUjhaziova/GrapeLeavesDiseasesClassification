{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "### 1. Setup\n\nTo prepare your environment, you need to install some packages and enter credentials for the Watson services.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### 1.1 Install the necessary packages", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Install IBM Cloud Object Storage Client:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install --upgrade setuptools "
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install ibm-cos-sdk"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install --upgrade tensorflow"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install --upgrade scikit-image"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install pycm==2.3"
        }, 
        {
            "source": "#### Now restart the kernel by choosing Kernel > Restart.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1.2 Import packages and libraries\n\nImport the packages and libraries that you'll use:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2. Configuration\n\nAdd configurable items of the notebook below", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2.1 Add your service credentials for Object Storage\n\nYou must create Object Storage service on IBM Cloud. To access data in a file in Object Storage, you need the Object Storage authentication credentials. Insert the Object Storage Streaming Body credentials and ensure the variable is referred as streaming_body_1 in the following cell after removing the current contents in the cell.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
                }
            ], 
            "source": "import sys\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport PIL\nimport keras\nimport itertools\nfrom PIL import Image\nimport ibm_boto3\nfrom botocore.client import Config\nfrom IPython.display import clear_output\nfrom pycm import *\n\nfrom sklearn.utils import shuffle\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import preprocessing\nfrom skimage import feature, data, io, measure, transform\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n%matplotlib inline \n"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "### 2.2 Global Variables\n\nEnter the batch size for training, testing and validation dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "\"original\\nbatch_size_train = 20\\nbatch_size_test = 15\\nbatch_size_val = 10\\nnum_classes= 4\\n#intereseted_folder='Documents' \\nSTANDARD_SIZE=(224,224)\\n\""
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "'''original\nbatch_size_train = 20\nbatch_size_test = 15\nbatch_size_val = 10\nnum_classes= 4\n#intereseted_folder='Documents' \nSTANDARD_SIZE=(224,224)\n'''"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "batch_size_train = 20\nbatch_size_test = 12\nbatch_size_val = 24\n\nnum_classes= 4\nSTANDARD_SIZE=(224,224)"
        }, 
        {
            "source": "## 3. Storage\n\n### 3.1 Extract the Dataset\n\nInput the zip file from object storage and extract the data onto the /home/dsxuser/work folder", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['dry', 'esca', 'healthy', 'mite']\n"
                }
            ], 
            "source": "from io import BytesIO\nimport zipfile\n\nzip_ref = zipfile.ZipFile(BytesIO(streaming_body_1.read()),'r')\npaths = zip_ref.namelist()\nclasses_required=[]\nfor path in paths:\n    zip_ref.extract(path)\n    temp=path.split('/')\n    if len(temp) > 3:\n        if temp[2] not in classes_required:\n            classes_required.append(temp[2])\nprint(classes_required)\nzip_ref.close()"
        }, 
        {
            "source": "## 4. Classification\n\n### 4.1 Create the Dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'''Converting Data Format according to the backend used by Keras\n'''\ndatagen=keras.preprocessing.image.ImageDataGenerator(data_format=K.image_data_format())"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Found 200 images belonging to 4 classes.\n"
                }, 
                {
                    "execution_count": 12, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "keras.preprocessing.image.DirectoryIterator"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "'''Input the Training Data'''\ntrain_path = '/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512/training_set/'\ntrain_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(train_path, target_size=(224,224), classes=classes_required, batch_size=batch_size_train, shuffle=False)\ntype(train_batches)"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Found 48 images belonging to 4 classes.\n"
                }
            ], 
            "source": "'''Input the Test Data'''\ntest_path = '/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512/test_set/'\ntest_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(test_path, target_size=(224,224), classes=classes_required, batch_size=batch_size_test, shuffle=False)"
        }, 
        {
            "execution_count": 54, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Found 48 images belonging to 4 classes.\n"
                }
            ], 
            "source": "test_batches_pred = ImageDataGenerator(rescale = 1./255).flow_from_directory(test_path, target_size=(224,224), classes=classes_required, batch_size=48, shuffle=False)"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Found 24 images belonging to 4 classes.\n"
                }
            ], 
            "source": "'''Input the Validation Data'''\nval_path = '/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512/validation_set/'\nval_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(val_path, target_size=(224,224), classes=classes_required, batch_size=batch_size_val, shuffle=False)"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "train_imgs, train_labels = next(train_batches)\ntrain_labels"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 16, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_train= [ np.where(r==1)[0][0] for r in train_labels ]\ny_train"
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 55, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "test_imgs, test_labels = next(test_batches_pred)\ntest_labels"
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 56, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_test= [ np.where(r==1)[0][0] for r in test_labels ]\ny_test"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val_imgs, val_labels = next(val_batches)\nval_labels"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 20, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_val= [ np.where(r==1)[0][0] for r in val_labels ]\ny_val"
        }, 
        {
            "source": "### 4.2 Build the Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ], 
            "source": "vgg16_model = keras.applications.vgg16.VGG16()\nvgg16_model.summary()"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(vgg16_model) #This is a Keras Functional API need to convert to sequential\nmodel = Sequential() #Iterate over the functional layers and add it as a stack\nfor layer in vgg16_model.layers[:-1]:\n    model.add(layer)"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ], 
            "source": "model.summary()"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for layer in model.layers: #Since the model is already trained with certain weights, we dont want to change it. Let it be the same\n    layer.trainable = False"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\ndense_1 (Dense)              (None, 4)                 16388     \n=================================================================\nTotal params: 134,276,932\nTrainable params: 16,388\nNon-trainable params: 134,260,544\n_________________________________________________________________\n"
                }
            ], 
            "source": "model.add(Dense(4, activation='softmax')) # Add the last layer\nmodel.summary()"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "class PlotLearning(Callback):\n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        self.acc = []\n        self.val_acc = []\n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.acc.append(logs.get('acc'))\n        self.val_acc.append(logs.get('val_acc'))\n        self.i += 1\n        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n        \n        clear_output(wait=True)\n        \n        ax1.set_yscale('log')\n        ax1.plot(self.x, self.losses, label=\"loss\")\n        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n        ax1.legend()\n        \n        ax2.plot(self.x, self.acc, label=\"accuracy\")\n        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n        ax2.legend()\n        \n        plt.show();\n        \nplot_learning = PlotLearning()"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Complie the model\nmodel.compile(Adam(lr=.00015), loss='categorical_crossentropy', metrics=['accuracy'])"
        }, 
        {
            "source": "### 4.3 Train the Model\n\nThe model will take about 30-45 minutes to train.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 28, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'\\nmodel.fit_generator(train_batches, steps_per_epoch=20, \\n                    validation_data=test_batches, validation_steps=12, epochs=2, verbose=1, callbacks=[plot_losses])        \\n'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "'''\nmodel.fit_generator(train_batches, steps_per_epoch=20, \n                    validation_data=test_batches, validation_steps=12, epochs=2, verbose=1, callbacks=[plot_losses])        \n'''                    "
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "image/png": "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\n", 
                        "text/plain": "<matplotlib.figure.Figure at 0x7f480c7fd278>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 29, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f480cac5908>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.fit_generator(train_batches, steps_per_epoch=20, \n                    validation_data=test_batches, validation_steps=12, epochs=2, verbose=1, shuffle=False, callbacks=[plot_learning]) "
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#model.fit_generator(train_batches, steps_per_epoch=20, validation_split=0.2, shuffle=True, validation_steps=12, epochs=10, verbose=1)"
        }, 
        {
            "source": "### 4.4 Test the Model ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_result_path = \"/home/dsxuser/work/class_4_CNN_model_DvsEvsHvsM_224_224_upd.h5\"\nmodel.save(model_result_path)"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.save_weights('/home/dsxuser/work/4_class_CNN_model_DvsEvsHvsM_224-224_upd_weights.h5')"
        }, 
        {
            "source": "Functions for testing the model.\n\npredict_generator takes your test data and gives you the output.\n\nevaluate_generator uses both your test input and output. It first predicts output using training input and then evaluates performance by comparing it against your test output. So it gives out a measure of performance, i.e. accuracy in your case.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Loss =  1.3203969121 , Accuracy =  0.295000001416\n"
                }
            ], 
            "source": "scores_train = model.evaluate_generator(generator = train_batches, steps=10)\nprint(\"Loss = \", scores_train[0], \", Accuracy = \", scores_train[1])"
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Loss =  1.31075373292 , Accuracy =  0.333333333954\n"
                }
            ], 
            "source": "scores_test = model.evaluate_generator(generator = test_batches, steps=4)\nprint(\"Loss = \", scores_test[0], \", Accuracy = \", scores_test[1])"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Loss =  1.30921852589 , Accuracy =  0.291666656733\n"
                }
            ], 
            "source": "scores_val = model.evaluate_generator(generator = val_batches, steps=1)\nprint(\"Loss = \", scores_val[0], \", Accuracy = \", scores_val[1])"
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\ndense_1 (Dense)              (None, 4)                 16388     \n=================================================================\nTotal params: 134,276,932\nTrainable params: 16,388\nNon-trainable params: 134,260,544\n_________________________________________________________________\n"
                }
            ], 
            "source": "model.summary()"
        }, 
        {
            "source": "### 4.4 Accuracy Testing", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For each of the images the notebook ran predictions and probability of each of them belonging to different classes were listed. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 57, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[ 0.23611222,  0.17134018,  0.27828249,  0.3142651 ],\n       [ 0.19133332,  0.14707668,  0.26344836,  0.39814168],\n       [ 0.18115109,  0.18703508,  0.29027724,  0.34153661],\n       [ 0.16635378,  0.29153916,  0.2236136 ,  0.31849349],\n       [ 0.23870319,  0.16993342,  0.2829406 ,  0.3084228 ],\n       [ 0.22400516,  0.20383482,  0.26116389,  0.31099612],\n       [ 0.23789901,  0.14709073,  0.2159334 ,  0.39907685],\n       [ 0.26864547,  0.15070394,  0.2260223 ,  0.35462832],\n       [ 0.22249292,  0.16803962,  0.21137546,  0.39809203],\n       [ 0.21431762,  0.13770689,  0.25246885,  0.39550665],\n       [ 0.18524811,  0.25658882,  0.2631295 ,  0.29503354],\n       [ 0.23083746,  0.21113394,  0.2678735 ,  0.29015511],\n       [ 0.16291729,  0.25792199,  0.30392554,  0.27523521],\n       [ 0.14213878,  0.25574797,  0.31992474,  0.28218842],\n       [ 0.17536776,  0.29568744,  0.27007306,  0.25887179],\n       [ 0.17073505,  0.31678766,  0.25941178,  0.25306559],\n       [ 0.17489024,  0.23442233,  0.32442588,  0.26626155],\n       [ 0.18269609,  0.284601  ,  0.27746937,  0.2552335 ],\n       [ 0.1322026 ,  0.20208184,  0.3499648 ,  0.31575078],\n       [ 0.16245058,  0.17620882,  0.23298392,  0.42835668],\n       [ 0.16115032,  0.2856569 ,  0.2423526 ,  0.31084019],\n       [ 0.18168549,  0.27773392,  0.25174338,  0.28883728],\n       [ 0.199848  ,  0.23747951,  0.22506584,  0.33760676],\n       [ 0.18674739,  0.21055815,  0.27885118,  0.32384321],\n       [ 0.15129261,  0.22340962,  0.27754939,  0.34774837],\n       [ 0.19509012,  0.26095247,  0.28447914,  0.25947824],\n       [ 0.21162727,  0.28039542,  0.24182989,  0.2661474 ],\n       [ 0.20341334,  0.23551929,  0.243798  ,  0.31726938],\n       [ 0.15518095,  0.18235043,  0.30857909,  0.35388952],\n       [ 0.14015643,  0.20749216,  0.31446883,  0.33788258],\n       [ 0.17120689,  0.17146459,  0.28808635,  0.36924216],\n       [ 0.1667698 ,  0.19838439,  0.27765611,  0.3571896 ],\n       [ 0.16107695,  0.13114282,  0.33708006,  0.37070015],\n       [ 0.13184676,  0.20500588,  0.30472353,  0.35842383],\n       [ 0.23689941,  0.21927679,  0.23852019,  0.3053036 ],\n       [ 0.17689241,  0.14626178,  0.23802878,  0.43881708],\n       [ 0.18992335,  0.21249025,  0.26687309,  0.33071336],\n       [ 0.25713181,  0.21955404,  0.20143026,  0.32188389],\n       [ 0.20331208,  0.16560376,  0.30356887,  0.3275153 ],\n       [ 0.17091167,  0.24843749,  0.24455999,  0.33609077],\n       [ 0.17670576,  0.20409639,  0.250211  ,  0.36898682],\n       [ 0.13155638,  0.15191416,  0.2514298 ,  0.46509966],\n       [ 0.14071438,  0.19458738,  0.27489001,  0.38980833],\n       [ 0.22386603,  0.21603176,  0.20773952,  0.35236263],\n       [ 0.14823423,  0.22088876,  0.30900249,  0.32187453],\n       [ 0.16502267,  0.21704884,  0.25665435,  0.36127409],\n       [ 0.17050579,  0.21524432,  0.26350167,  0.35074815],\n       [ 0.16127737,  0.20307486,  0.27413255,  0.36151528]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "predictions_test = model.predict_generator(test_batches_pred, steps=1, verbose=0)\npredictions_test"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 1, 2, 1, 2, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
                }
            ], 
            "source": "y_pred_test =[]\nfor i in range(len(predictions_test)):\n    y_pred_test.append(np.argmax(predictions_test[i]))\nprint(y_pred_test)"
        }, 
        {
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "33.33333333333333\n"
                }
            ], 
            "source": "ctr_test=0\nfor i in range(len(y_pred_test)):\n    if y_pred_test[i] == y_test[i]:\n        ctr_test=ctr_test+1\nres_test = ctr_test/len(y_pred_test)*100\nprint(res_test)"
        }, 
        {
            "execution_count": 38, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 38, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[ 0.24288145,  0.16380504,  0.29849261,  0.29482087],\n       [ 0.19357234,  0.18007784,  0.24147083,  0.38487899],\n       [ 0.23142181,  0.14601356,  0.21046667,  0.41209799],\n       [ 0.20414594,  0.14083537,  0.24588045,  0.40913823],\n       [ 0.22526628,  0.25215557,  0.2806887 ,  0.24188939],\n       [ 0.24273266,  0.16338061,  0.25816709,  0.33571959],\n       [ 0.19326545,  0.29041484,  0.25269026,  0.26362947],\n       [ 0.17190358,  0.21076033,  0.28552014,  0.33181587],\n       [ 0.13838802,  0.22690912,  0.29950634,  0.33519652],\n       [ 0.14324996,  0.21448196,  0.3242341 ,  0.31803402],\n       [ 0.15252864,  0.19690932,  0.22471903,  0.425843  ],\n       [ 0.15756264,  0.29110727,  0.29818639,  0.2531437 ],\n       [ 0.18994531,  0.21770047,  0.27726376,  0.31509042],\n       [ 0.14638534,  0.25953916,  0.27971572,  0.31435975],\n       [ 0.20702374,  0.28656936,  0.23535125,  0.27105564],\n       [ 0.15627602,  0.26387805,  0.27673879,  0.30310705],\n       [ 0.12093802,  0.22478762,  0.29120749,  0.36306688],\n       [ 0.13287905,  0.16010702,  0.32914743,  0.37786651],\n       [ 0.20792398,  0.17655303,  0.27655336,  0.33896971],\n       [ 0.2314404 ,  0.18316954,  0.23733762,  0.34805244],\n       [ 0.20347542,  0.16848905,  0.30931723,  0.31871837],\n       [ 0.19105022,  0.19022065,  0.23963349,  0.37909567],\n       [ 0.17257251,  0.20939702,  0.21188773,  0.40614271],\n       [ 0.14253588,  0.20933558,  0.25951222,  0.38861626]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "predictions_val = model.predict_generator(val_batches, steps=1, verbose=0)\npredictions_val"
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[2, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
                }
            ], 
            "source": "y_pred_val=[]\nfor i in range(len(predictions_val)):\n    y_pred_val.append(np.argmax(predictions_val[i]))\nprint(y_pred_val)"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "29.166666666666668\n"
                }
            ], 
            "source": "#plots(val_imgs, titles=y_pred_val)\nctr=0\nfor i in range(len(y_pred_val)):\n    if y_pred_val[i] == y_val[i]:\n        ctr=ctr+1\nres = ctr/len(y_pred_val)*100\nprint(res)"
        }, 
        {
            "source": "### 4.5 Confusion matrix", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Confusion matrix for test dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 124, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# confusion matrix for test data\ny_actu_test = y_test"
        }, 
        {
            "execution_count": 125, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y_pred_test = y_pred_test"
        }, 
        {
            "execution_count": 130, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cm_test = ConfusionMatrix(actual_vector=y_actu_test, predict_vector=y_pred_test) "
        }, 
        {
            "execution_count": 131, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 131, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0, 1, 2, 3]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "cm_test.classes"
        }, 
        {
            "execution_count": 132, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 132, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{0: {0: 0, 1: 0, 2: 0, 3: 12},\n 1: {0: 0, 1: 3, 2: 4, 3: 5},\n 2: {0: 0, 1: 1, 2: 1, 3: 10},\n 3: {0: 0, 1: 0, 2: 0, 3: 12}}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "cm_test.table"
        }, 
        {
            "execution_count": 143, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Predict  0        1        2        3        \nActual\n0        0        0        0        12       \n\n1        0        3        4        5        \n\n2        0        1        1        10       \n\n3        0        0        0        12       \n\n\n"
                }
            ], 
            "source": "cm_test.print_matrix()"
        }, 
        {
            "source": "Confusion matrix for validation dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 134, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# confusion matrix for test data\ny_actu_val = y_val"
        }, 
        {
            "execution_count": 135, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y_pred_val = y_pred_val"
        }, 
        {
            "execution_count": 136, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cm_val = ConfusionMatrix(actual_vector=y_actu_val, predict_vector=y_pred_val) "
        }, 
        {
            "execution_count": 137, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 137, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0, 1, 2, 3]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "cm_val.classes"
        }, 
        {
            "execution_count": 138, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 138, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{0: {0: 0, 1: 0, 2: 2, 3: 4},\n 1: {0: 0, 1: 1, 2: 2, 3: 3},\n 2: {0: 0, 1: 1, 2: 0, 3: 5},\n 3: {0: 0, 1: 0, 2: 0, 3: 6}}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "cm_val.table"
        }, 
        {
            "execution_count": 142, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Predict 0       1       2       3       \nActual\n0       0       0       2       4       \n\n1       0       1       2       3       \n\n2       0       1       0       5       \n\n3       0       0       0       6       \n\n\n"
                }
            ], 
            "source": "#print(cm_val)\ncm_val.print_matrix()"
        }, 
        {
            "source": "### 4.6 Test the Model with External Test Images", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 61, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.load_weights('4_class_CNN_model_DvsEvsHvsM_224-224_upd_weights.h5')"
        }, 
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about your possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\nstreaming_body_2 = client_f33a0d38cc5d40cfbdc47a3041a4488b.get_object(Bucket='savethevineyard-donotdelete-pr-rrkbguhydjhuu6', Key='datasetWS_DvsEvsHvsM_512-512_ext_set.zip')['Body']\n# add missing __iter__ method so pandas accepts body as file-like object\nif not hasattr(streaming_body_2, \"__iter__\"): streaming_body_2.__iter__ = types.MethodType( __iter__, streaming_body_2 ) \n\n"
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['datasetWS_DvsEvsHvsM_512-512_ext_set/dry.testset.08-2.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/dry.validset.01.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/dry.validset.05.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/dry_06-2.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/dry_21.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/esca.IMG_7508.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/esca.IMG_7516.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/esca.testset.7544_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/esca.testset.7548_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/esca.testset.7549_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/esca.validset.7557_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.IMG_7666.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.IMG_7671_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7683_1-2.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7688_1-2.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7735_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.validset.8045.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/mite.IMG_7594_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7562.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7563.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7565_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7568_1.jpg', 'datasetWS_DvsEvsHvsM_512-512_ext_set/mite.validset.7565_2.jpg']\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/dry.testset.08-2.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/dry.validset.01.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/dry.validset.05.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/dry_06-2.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/dry_21.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/esca.IMG_7508.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/esca.IMG_7516.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/esca.testset.7544_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/esca.testset.7548_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/esca.testset.7549_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/esca.validset.7557_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.IMG_7666.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.IMG_7671_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7683_1-2.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7688_1-2.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7735_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.validset.8045.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/mite.IMG_7594_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7562.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7563.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7565_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/mite.testset.7568_1.jpg\n/home/dsxuser/work/datasetWS_DvsEvsHvsM_512-512_ext_set/mite.validset.7565_2.jpg\n"
                }
            ], 
            "source": "from io import BytesIO\nimport zipfile\n\nzip_ref = zipfile.ZipFile(BytesIO(streaming_body_2.read()),'r')\npaths = zip_ref.namelist()\ndel paths[0]\nprint(paths)\nfor path in paths:\n    print(zip_ref.extract(path))\nzip_ref.close()"
        }, 
        {
            "execution_count": 113, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_test_ext=[]\ndef convert_to_image(X):\n    '''Function to convert all Input Images to the STANDARD_SIZE and create Validation Dataset\n    '''\n    for f in paths:\n        #fobj=get_file(f)\n        #print(type(fobj))predictions= model.predict(X_test)\n        if os.path.isdir(f):\n            continue\n        img= PIL.Image.open(f)\n        img = img.resize(STANDARD_SIZE)\n        img=np.array(img)\n        img= img.astype('float32') / 255.0\n        X.append(img)        \n    return X\nX_test_ext=np.array(convert_to_image(X_test_ext))\ndatagen.fit(X_test_ext)"
        }, 
        {
            "source": "For each of the images the notebook ran predictions and probability of each of them belonging to different classes were listed. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 114, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 114, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[ 0.19357237,  0.18007781,  0.2414708 ,  0.38487899],\n       [ 0.23142184,  0.14601365,  0.21046665,  0.41209787],\n       [ 0.20414589,  0.14083546,  0.2458804 ,  0.40913823],\n       [ 0.22526632,  0.25215557,  0.28068861,  0.24188943],\n       [ 0.24273267,  0.16338064,  0.25816712,  0.33571953],\n       [ 0.19326538,  0.29041487,  0.25269029,  0.26362944],\n       [ 0.1719036 ,  0.2107603 ,  0.28552017,  0.33181593],\n       [ 0.13838799,  0.22690921,  0.29950631,  0.33519652],\n       [ 0.14324988,  0.21448202,  0.32423416,  0.31803393],\n       [ 0.15252863,  0.19690926,  0.22471896,  0.42584312],\n       [ 0.15756267,  0.29110727,  0.29818636,  0.25314373],\n       [ 0.18994531,  0.21770048,  0.27726379,  0.31509039],\n       [ 0.14638545,  0.2595391 ,  0.27971563,  0.31435984],\n       [ 0.2070238 ,  0.28656936,  0.23535125,  0.27105564],\n       [ 0.15627609,  0.26387808,  0.27673876,  0.30310702],\n       [ 0.12093803,  0.22478762,  0.29120755,  0.36306676],\n       [ 0.13287902,  0.16010693,  0.3291474 ,  0.3778666 ],\n       [ 0.20792384,  0.17655303,  0.27655327,  0.33896983],\n       [ 0.23144035,  0.18316966,  0.23733757,  0.3480525 ],\n       [ 0.20347546,  0.16848905,  0.30931717,  0.31871834],\n       [ 0.19105025,  0.19022055,  0.23963343,  0.37909582],\n       [ 0.17257258,  0.20939705,  0.21188772,  0.40614271],\n       [ 0.14253594,  0.20933561,  0.25951228,  0.38861623]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "predictions_ext= model.predict(X_test_ext)\npredictions_ext"
        }, 
        {
            "execution_count": 115, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 115, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_pred_ext=[]\nfor i in range(len(predictions_ext)):\n    y_pred_ext.append(np.argmax(predictions_ext[i]))\ny_pred_ext"
        }, 
        {
            "execution_count": 101, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "datasetWS_DvsEvsHvsM_512-512_val_set/dry.testset.08-2.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/dry.validset.01.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/dry.validset.05.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/dry_06-2.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/dry_21.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/esca.IMG_7508.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/esca.IMG_7516.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/esca.testset.7544_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/esca.testset.7548_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/esca.testset.7549_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/esca.validset.7557_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/healthy.IMG_7666.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/healthy.IMG_7671_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/healthy.testset.7683_1-2.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/healthy.testset.7688_1-2.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/healthy.testset.7735_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/healthy.validset.8045.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/mite.IMG_7594_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7562.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7563.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7565_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7568_1.jpg\ndatasetWS_DvsEvsHvsM_512-512_val_set/mite.validset.7565_2.jpg\n"
                }
            ], 
            "source": "for i in range(len(y_pred_ext)):\n    print(paths[i])"
        }, 
        {
            "execution_count": 116, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['dry', 'esca', 'healthy', 'mite']\n"
                }
            ], 
            "source": "print(classes_required)"
        }, 
        {
            "execution_count": 117, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "index= classes_required.index('dry')\nfor i in range(len(y_pred_ext)):\n    if y_pred_ext[i] == index:\n        print(\"Image classified as a dry: \", paths[i])"
        }, 
        {
            "execution_count": 118, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Image classified as a esca:  datasetWS_DvsEvsHvsM_512-512_ext_set/esca.IMG_7508.jpg\nImage classified as a esca:  datasetWS_DvsEvsHvsM_512-512_ext_set/healthy.testset.7683_1-2.jpg\n"
                }
            ], 
            "source": "index= classes_required.index('esca')\nfor i in range(len(y_pred_ext)):\n    if y_pred_ext[i] == index:\n        print(\"Image classified as a esca: \", paths[i])"
        }, 
        {
            "execution_count": 76, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Image classified as a healthy:  datasetWS_DvsEvsHvsM_512-512_val_set/dry_06-2.jpg\nImage classified as a healthy:  datasetWS_DvsEvsHvsM_512-512_val_set/esca.testset.7548_1.jpg\nImage classified as a healthy:  datasetWS_DvsEvsHvsM_512-512_val_set/esca.validset.7557_1.jpg\n"
                }
            ], 
            "source": "index= classes_required.index('healthy')\nfor i in range(len(y_pred_ext)):\n    if y_pred_ext[i] == index:\n        print(\"Image classified as a healthy: \", paths[i])"
        }, 
        {
            "execution_count": 77, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Image classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/dry.testset.08-2.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/dry.validset.01.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/dry.validset.05.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/dry_21.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/esca.IMG_7516.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/esca.testset.7544_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/esca.testset.7549_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/healthy.IMG_7666.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/healthy.IMG_7671_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/healthy.testset.7688_1-2.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/healthy.testset.7735_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/healthy.validset.8045.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/mite.IMG_7594_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7562.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7563.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7565_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/mite.testset.7568_1.jpg\nImage classified as a mite:  datasetWS_DvsEvsHvsM_512-512_val_set/mite.validset.7565_2.jpg\n"
                }
            ], 
            "source": "index= classes_required.index('mite')\nfor i in range(len(y_pred_ext)):\n    if y_pred_ext[i] == index:\n        print(\"Image classified as a mite: \", paths[i])"
        }, 
        {
            "source": "## 5. Persist the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this section, you will show how to use the watson-machine-learning-client package to store our model in the WML repository.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!rm -rf $PIP_BUILD/watson-machine-learning-client"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install watson-machine-learning-client"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wml_credentials={\n    'url': 'https://eu-gb.ml.cloud.ibm.com',\n    'access_key': 'ogsy6k3b_bBFms5UiyFCq9KSjLTBLiT_KDl4fqd8ePYX',\n    'username': '93d8a4fc-12bf-4c2a-ad01-3cfb62331a34',\n    'password': 'f7283f45-23da-4699-a513-983d401b0f4f',\n    'instance_id': '98080101-8a86-4aab-84ac-a6dec0b0bd27'\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient(wml_credentials)"
        }, 
        {
            "source": "### 5.1 Save the CNN model in the WML Repository", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Save the model artifact as CNN model for image classification to WML instance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Step 1: Save your model to .h5 file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_result_path = \"/home/dsxuser/work/class_4_CNN_model_DvsEvsHvsM_224_224_upd.h5\"\nmodel.save(model_result_path)"
        }, 
        {
            "source": "#### Step 2: compress .h5 file to tgz.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!tar -zcvf class_4_CNN_model_DvsEvsHvsM_224_224_upd.tgz class_4_CNN_model_DvsEvsHvsM_224_224_upd.h5"
        }, 
        {
            "source": "#### Step 3: Important > For deploying a keras model, it is mandatory to pass the FRAMEWORK_LIBRARIES along with other metaprops", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "metadata = {\n         client.repository.ModelMetaNames.NAME: 'CNN model upd for grape leaves disease classification',\n         client.repository.ModelMetaNames.FRAMEWORK_NAME: 'tensorflow',\n         client.repository.ModelMetaNames.FRAMEWORK_VERSION: '1.5',\n         client.repository.ModelMetaNames.RUNTIME_NAME: 'python',\n         client.repository.ModelMetaNames.RUNTIME_VERSION: '3.5',\n         client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{'name':'keras', 'version': '2.1.3'}]\n}"
        }, 
        {
            "source": "#### Step 4: Store the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "published_model = client.repository.store_model(model='class_4_CNN_model_DvsEvsHvsM_224_224_upd.tgz', meta_props=metadata)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Display a list of all the models.\nclient.repository.list_models()"
        }, 
        {
            "source": "### 5.2 Deploy the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We need the model uid to create the deployment. You can extract the model uid from the saved model details.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Step 5: Deploy the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We need the model uid to create the deployment. You can extract the model uid from the saved model details.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Extract the uid.\nmodel_uid = client.repository.get_model_uid(published_model)\nprint(model_uid)"
        }, 
        {
            "source": "We use this modul_uid in the next section to create the deployment.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Step 6: Create the deployment", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now, we can create a deployment, and classify grape leaves diseases.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create the deployment.\ndeployment_details = client.deployments.create(model_uid, 'Predict grape leaves diseases - CNN model upd')"
        }, 
        {
            "source": "How to score data using the deployable model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def my_deployable_function():\n\n    def score( payload ):\n    \n        return payload[\"values\"][0]\n\n    return score\n\nfunction_result = my_deployable_function()( { \"values\" : [ \"Hello, world!\" ] } )\nprint( function_result )\n\n# Look up your Watson Machine Learning credentials and then paste them here\nwml_credentials = {\n    \"apikey\"      : \"\",\n    \"instance_id\" : \"\",\n    \"password\"    : \"\",\n    \"url\"         : \"\",\n    \"username\"    : \"\"\n}\n\n# Instantiate WatsonMachineLearningAPIClient\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient\nclient = WatsonMachineLearningAPIClient( wml_credentials )\n\n# Store the function\nmeta_data = { client.repository.FunctionMetaNames.NAME : 'My deployable function' }\nfunction_details = client.repository.store_function( meta_props=meta_data, function=my_deployable_function \n\nfunction_id = function_details[\"metadata\"][\"guid\"]\nfunction_deployment_details = client.deployments.create( artifact_uid=function_id, name='My function deployment' )\n\nExample 1: Python code, using the Python client\n\n\nfunction_deployment_endpoint_url = client.deployments.get_scoring_url( function_deployment_details )\npayload = { \"values\" : [ \"Hello, world!\" ] }\n\n# Send data to deployment for processing\nclient.deployments.score( function_deployment_endpoint_url, payload )\n\nimport requests\n\n# Get an IAM token from IBM Cloud\nurl     = \"https://iam.bluemix.net/oidc/token\"\nheaders = { \"Content-Type\" : \"application/x-www-form-urlencoded\" }\ndata    = \"apikey=\" + wml_credentials[\"apikey\"] + \"&grant_type=urn:ibm:params:oauth:grant-type:apikey\"\nIBM_cloud_iam_uid = \"bx\"\nIBM_cloud_iam_pwd = \"bx\"\nresponse  = requests.post( url, headers=headers, data=data, auth=( IBM_cloud_iam_uid, IBM_cloud_iam_pwd ) )\n\nif 200 != response.status_code:\n    print( response.status_code )\n    print( response.reason )\nelse:\n    iam_token = response.json()[\"access_token\"]\n    \n    # Send data to deployment for processing\n    headers = { \"Content-Type\" : \"application/json\", \n                \"Authorization\" : \"Bearer \" + iam_token,\n                \"ML-Instance-ID\" : wml_credentials[\"instance_id\"] }\n    response = requests.post( function_deployment_endpoint_url, json=payload, headers=headers )\n    print ( response.text )\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "build_time_var = { \"text\" : \"Hello again.\" }\n\ndef my_deployable_function( parms=build_time_var ):\n\n    def score( payload ):\n    \n        return payload[\"values\"][0] + \"\\n\" + parms[\"text\"]\n\n    return score"
        }, 
        {
            "source": "Another example of fuction creation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def my_deployable_function():\n\n    def score( payload ):\n    \n        from PIL import Image\n        \n        return payload[\"values\"][0]\n\n    return score"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def my_deployable_function():\n    \n    import subprocess\n    subprocess.check_output( \"pip install PIL --user\", stderr=subprocess.STDOUT, shell=True )        \n    \n    def getRGBAArr( canvas_data ):\n        import numpy as np\n        dimension = canvas_data[\"height\"]\n        rgba_data = canvas_data[\"data\"]\n        rgba_arr  = np.asarray( rgba_data ).astype('uint8')\n        return rgba_arr.reshape( dimension, dimension, 4 )\n\n    def getNormAlphaList( img ):\n        import numpy as np\n        alpha_arr       = np.array( img.split()[-1] )\n        norm_alpha_arr  = alpha_arr / 255\n        norm_alpha_list = norm_alpha_arr.reshape( 1, 784 ).tolist()\n        return norm_alpha_list\n    \n    def score( function_payload ):\n\n        from PIL import Image\n        canvas_data   = function_payload[\"values\"][0]           # Read the payload received by the function\n        rgba_arr      = getRGBAArr( canvas_data )               # Create an array object with the required shape\n        img           = Image.fromarray( rgba_arr, 'RGBA' )     # Create an image object that can be resized\n        sm_img        = img.resize( ( 28, 28 ), Image.LANCZOS ) # Resize the image to 28 x 28 pixels\n        alpha_list    = getNormAlphaList( sm_img )              # Create a 1 x 784 array of values between 0 and 1\n        model_payload = { \"values\" : alpha_list }               # Create a payload to be sent to the model\n            \n        # Send model_payload to the model deployment here ...\n        # model_result is what is returned ...\n        #\n        return model_result\n\n\n    return score\n"
        }, 
        {
            "source": "Example Python code\n\nThis example shows how a deployed function can prostprocess the results returned from a model deployment, and then return simpler output to your application (a single digit class, instead of the whole JSON-formated result):", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def my_deployable_function( parms=ai_parms ):\n    \n    from watson_machine_learning_client import WatsonMachineLearningAPIClient\n    client = WatsonMachineLearningAPIClient( parms[\"wml_credentials\"] )\n    \n    def score( payload ):\n    \n        model_result = client.deployments.score( parms[\"model_endpoint_url\"], payload )\n        \n        return model_result[\"values\"][0]\n    \n    return score"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ai_parms = { \"wml_credentials\" : wml_credentials, \"model_deployment_endpoint_url\" : model_deployment_endpoint_url }\n\ndef my_deployable_function( parms=ai_parms ):\n    \n    try:\n        \n        import subprocess\n        subprocess.check_output( \"pip install Pillow --user\", stderr=subprocess.STDOUT, shell=True )\n        \n    except subprocess.CalledProcessError as e:        \n        install_err = \"subprocess.CalledProcessError:\\n\\n\" + \"cmd:\\n\" + e.cmd + \"\\n\\noutput:\\n\" + e.output.decode()\n        raise Exception( \"Installing failed:\\n\" + install_err )\n    \n        \n    def getRGBAArr( canvas_data ):\n        import numpy as np\n        dimension = canvas_data[\"height\"]\n        rgba_data = canvas_data[\"data\"]\n        rgba_arr  = np.asarray( rgba_data ).astype('uint8')\n        return rgba_arr.reshape( dimension, dimension, 4 )\n        \n    def getNormAlphaList( img ):\n        import numpy as np\n        alpha_arr       = np.array( img.split()[-1] )\n        norm_alpha_arr  = alpha_arr / 255\n        norm_alpha_list = norm_alpha_arr.reshape( 1, 784 ).tolist()\n        return norm_alpha_list\n        \n    def score( function_payload ):\n            \n        try:\n            \n            from PIL import Image\n            canvas_data   = function_payload[\"values\"][0]           # Read the payload received by the function\n            rgba_arr      = getRGBAArr( canvas_data )               # Create an array object with the required shape\n            img           = Image.fromarray( rgba_arr, 'RGBA' )     # Create an image object that can be resized\n            sm_img        = img.resize( ( 28, 28 ), Image.LANCZOS ) # Resize the image to 28 x 28 pixels\n            alpha_list    = getNormAlphaList( sm_img )              # Create a 1 x 784 array of values between 0 and 1\n            model_payload = { \"values\" : alpha_list }               # Create a payload to be sent to the model\n                    \n            #print( \"Payload for model:\" ) # For debugging purposes\n            #print( model_payload )        # For debugging purposes\n                    \n            from watson_machine_learning_client import WatsonMachineLearningAPIClient\n            client       = WatsonMachineLearningAPIClient( parms[\"wml_credentials\"] )\n            model_result = client.deployments.score( parms[\"model_deployment_endpoint_url\"], model_payload )\n            digit_class  = model_result[\"values\"][0]\n                    \n            return { \"digit_class\" : digit_class }\n            \n        except Exception as e:\n            \n            return { \"error\" : repr( e ) }\n\n\n    return score"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}